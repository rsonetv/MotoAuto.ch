# ğŸ¤– Integracja Groq SDK w Next.js - Kompletny Przewodnik

## ğŸ“‹ **Raport Weryfikacji - ZAKOÅƒCZONE**

### âœ… **ZREALIZOWANE ZADANIA:**

#### 1ï¸âƒ£ **PoÅ‚Ä…czenie z Vercel**
- âœ… Projekt juÅ¼ poÅ‚Ä…czony z Vercel
- âœ… Environment variables skonfigurowane
- âœ… `vercel link` i `vercel env pull` dostÄ™pne

#### 2ï¸âƒ£ **Instalacja Groq SDK**
- âœ… `groq-sdk` zainstalowany w dependencies
- âœ… Groq client zainicjalizowany w `lib/groq.ts`
- âœ… TypeScript types skonfigurowane

#### 3ï¸âƒ£ **API Routes**
- âœ… `/api/groq-test` endpoint utworzony
- âœ… GET i POST metody zaimplementowane
- âœ… Rate limiting z Upstash Redis
- âœ… Proper error handling

#### 4ï¸âƒ£ **Konfiguracja Environment**
- âœ… `GROQ_API_KEY` dodany do `.env.example`
- âœ… Walidacja z Zod schemas
- âœ… Security measures implemented

---

## ğŸš€ **Instrukcje Uruchomienia**

### **1. Konfiguracja Projektu**

\`\`\`bash
# JeÅ›li nowy projekt
npx create-next-app@latest my-groq-app --typescript --tailwind --eslint --app

# JeÅ›li istniejÄ…cy projekt
vercel login
vercel link
\`\`\`

### **2. Pobranie Environment Variables**

\`\`\`bash
# Pobierz zmienne z Vercel
npx vercel env pull .env.development.local

# Lub skopiuj rÄ™cznie
cp .env.example .env.local
\`\`\`

### **3. Instalacja Dependencies**

\`\`\`bash
# Zainstaluj Groq SDK
npm install groq-sdk

# Zainstaluj pozostaÅ‚e zaleÅ¼noÅ›ci
npm install @upstash/redis @upstash/ratelimit zod
\`\`\`

### **4. Konfiguracja Environment Variables**

Dodaj do `.env.local`:

\`\`\`bash
# Groq API
GROQ_API_KEY=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

# Upstash Redis (dla rate limiting)
KV_REST_API_URL=https://engaged-civet-46202.upstash.io
KV_REST_API_TOKEN=AbR6AAIjcDEyY2JkMmM0N2RhZGI0YzMwYWIxZDcwNWE4OGJhNGJiOXAxMA

# App URL
NEXT_PUBLIC_APP_URL=http://localhost:3000
\`\`\`

### **5. Uruchomienie Aplikacji**

\`\`\`bash
# Uruchom dev server
npm run dev

# Aplikacja dostÄ™pna na
http://localhost:3000
\`\`\`

---

## ğŸ§ª **Testowanie API**

### **Test 1: Podstawowy GET Request**

\`\`\`bash
curl http://localhost:3000/api/groq-test
\`\`\`

**Oczekiwana odpowiedÅº:**
\`\`\`json
{
  "success": true,
  "message": "CzeÅ›Ä‡! MiÅ‚o CiÄ™ poznaÄ‡. Jak mogÄ™ Ci dzisiaj pomÃ³c?",
  "model": "llama-3.3-70b-versatile",
  "timestamp": "2024-01-20T10:30:00.000Z"
}
\`\`\`

### **Test 2: POST Request z Custom Prompt**

\`\`\`bash
curl -X POST http://localhost:3000/api/groq-test \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Napisz krÃ³tki wiersz o programowaniu w Next.js",
    "model": "llama-3.3-70b-versatile"
  }'
\`\`\`

**Oczekiwana odpowiedÅº:**
\`\`\`json
{
  "success": true,
  "message": "Next.js to framework wspaniaÅ‚y,\nReact i SSR w jednym miejscu...",
  "model": "llama-3.3-70b-versatile",
  "prompt": "Napisz krÃ³tki wiersz o programowaniu w Next.js",
  "timestamp": "2024-01-20T10:30:00.000Z",
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 45,
    "total_tokens": 60
  }
}
\`\`\`

### **Test 3: Rate Limiting**

\`\`\`bash
# WyÅ›lij 15 requestÃ³w szybko (limit: 10/min)
for i in {1..15}; do
  curl -X POST http://localhost:3000/api/groq-test \
    -H "Content-Type: application/json" \
    -d '{"prompt":"Test '$i'"}' &
done
\`\`\`

**Po przekroczeniu limitu:**
\`\`\`json
{
  "error": "Rate limit exceeded. Try again in a minute.",
  "timestamp": "2024-01-20T10:30:00.000Z"
}
\`\`\`

---

## ğŸ“ **Struktura PlikÃ³w**

\`\`\`
â”œâ”€â”€ app/api/groq-test/route.ts    # Groq API endpoint
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ groq.ts                   # Groq client i helpers
â”‚   â”œâ”€â”€ ratelimit.ts              # Rate limiting
â”‚   â””â”€â”€ validations.ts            # Zod schemas
â”œâ”€â”€ .env.example                  # Environment variables template
â””â”€â”€ README-GROQ-INTEGRATION.md   # Ta dokumentacja
\`\`\`

---

## ğŸ”§ **DostÄ™pne Modele Groq**

| Model | Nazwa | Opis |
|-------|-------|------|
| `llama-3.3-70b-versatile` | Llama 3.3 70B | Najnowszy, najbardziej wszechstronny |
| `llama-3.1-70b-versatile` | Llama 3.1 70B | DuÅ¼y model, dobra jakoÅ›Ä‡ |
| `llama-3.1-8b-instant` | Llama 3.1 8B | Szybki, mniejszy model |
| `mixtral-8x7b-32768` | Mixtral 8x7B | Dobry dla dÅ‚ugich kontekstÃ³w |
| `gemma2-9b-it` | Gemma 2 9B | Zoptymalizowany dla instrukcji |

---

## ğŸ›¡ï¸ **BezpieczeÅ„stwo i Rate Limiting**

### **Rate Limits:**
- **Groq API**: 10 requestÃ³w / minutÄ™
- **General API**: 100 requestÃ³w / godzinÄ™
- **Auth endpoints**: 5 prÃ³b / 15 minut

### **Security Features:**
- Input validation z Zod
- Rate limiting z Upstash Redis
- Error handling bez exposure internal details
- IP-based limiting
- Request size limits

---

## ğŸ” **Monitoring i Debugging**

### **Logi w Development:**
\`\`\`bash
# SprawdÅº logi Next.js
npm run dev

# SprawdÅº logi Vercel
vercel logs
\`\`\`

### **Metryki do Åšledzenia:**
- Request count per endpoint
- Response times
- Error rates
- Rate limit hits
- Token usage (Groq)

---

## ğŸš€ **Deployment na Vercel**

### **1. Konfiguracja Environment Variables**

\`\`\`bash
# Dodaj zmienne do Vercel
vercel env add GROQ_API_KEY
vercel env add KV_REST_API_URL
vercel env add KV_REST_API_TOKEN
vercel env add NEXT_PUBLIC_APP_URL
\`\`\`

### **2. Deploy**

\`\`\`bash
# Deploy do production
vercel --prod

# SprawdÅº status
vercel ls
\`\`\`

### **3. Test Production Endpoint**

\`\`\`bash
# Test production API
curl https://your-app.vercel.app/api/groq-test
\`\`\`

---

## ğŸ”§ **Troubleshooting**

### **Problem: "Invalid API Key"**
\`\`\`bash
# SprawdÅº czy GROQ_API_KEY jest ustawiony
echo $GROQ_API_KEY

# SprawdÅº w Vercel Dashboard
vercel env ls
\`\`\`

### **Problem: Rate Limiting nie dziaÅ‚a**
\`\`\`bash
# SprawdÅº Redis connection
curl -X GET http://localhost:3000/api/redis

# SprawdÅº Upstash Dashboard
\`\`\`

### **Problem: Model nie odpowiada**
\`\`\`bash
# SprawdÅº dostÄ™pne modele
curl -X POST http://localhost:3000/api/groq-test \
  -d '{"prompt":"test","model":"invalid-model"}'

# UÅ¼yj domyÅ›lnego modelu
curl -X POST http://localhost:3000/api/groq-test \
  -d '{"prompt":"test"}'
\`\`\`

---

## ğŸ“Š **Performance Tips**

### **Optymalizacja RequestÃ³w:**
1. **UÅ¼yj odpowiedniego modelu** - `llama-3.1-8b-instant` dla szybkich odpowiedzi
2. **Ogranicz max_tokens** - ustaw rozsÄ…dny limit
3. **Cache responses** - dla powtarzajÄ…cych siÄ™ zapytaÅ„
4. **Batch requests** - grupuj podobne zapytania

### **Monitoring:**
\`\`\`typescript
// Dodaj do API route
console.log('Request:', { prompt, model, timestamp: new Date() })
console.log('Response time:', Date.now() - startTime, 'ms')
console.log('Tokens used:', result.data?.usage)
\`\`\`

---

## ğŸ¯ **NastÄ™pne Kroki**

### **Rozszerzenia:**
- [ ] Streaming responses dla dÅ‚ugich tekstÃ³w
- [ ] Chat interface z historiÄ…
- [ ] File upload i analiza dokumentÃ³w
- [ ] Integration z bazÄ… danych
- [ ] User-specific rate limiting
- [ ] Response caching
- [ ] Analytics dashboard
- [ ] A/B testing rÃ³Å¼nych modeli

### **Production Checklist:**
- [ ] Environment variables skonfigurowane
- [ ] Rate limiting przetestowany
- [ ] Error handling zweryfikowany
- [ ] Security review przeprowadzony
- [ ] Performance testing zakoÅ„czony
- [ ] Monitoring skonfigurowany
- [ ] Backup strategy ustalona

---

## ğŸ“ **Support**

W przypadku problemÃ³w:
1. SprawdÅº logi w Vercel Dashboard
2. SprawdÅº Groq API status
3. SprawdÅº Upstash Redis metrics
4. OtwÃ³rz issue na GitHub

**Groq SDK Integration - KOMPLETNY! âœ…**
